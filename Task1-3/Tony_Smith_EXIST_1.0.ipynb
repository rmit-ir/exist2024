{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "## Keeping your API Key hidden\n",
    "\n",
    "1. Create a new file and name it api_key.py.\n",
    "2. Add the API key to the file: API_KEY = \"your_api_key_here\"\n",
    "3. Save the file in the same directory where your main Python script is located.\n",
    "\n",
    "## Please look at [Configuration](Configuration) section below first before modifying code.\n",
    "- Task: Select the task\n",
    "- Eval: Evaluation type (soft or hard)\n",
    "- Prompt: If you would like to change the prompt from a text file instead of in the code.\n",
    "- API Key: Your imported API Key\n",
    "- Data Frame: Data Frame created to pass through in the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "from api_key import API_KEY\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_df(json_location, dataframe_name):\n",
    "    \"\"\"\n",
    "    Load a JSON file and convert it into a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    json_location (str): The file path of the JSON file.\n",
    "    dataframe_name (str): The name of the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The DataFrame created from the JSON data.\n",
    "    \"\"\"\n",
    "    with open(json_location, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    dataframe_name = pd.DataFrame.from_dict(data, orient='index')\n",
    "    return dataframe_name\n",
    "\n",
    "def load_prompt(file_path):\n",
    "    \"\"\"\n",
    "    Load the prompt from a file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file containing the prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The loaded prompt.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        prompt = file.readline().strip()\n",
    "    return prompt\n",
    "\n",
    "def save_responses(responses_dict, output_dir='.', base_filename='NO_FILE_NAME_RMIT', increment_filename=True):\n",
    "    \"\"\"\n",
    "    Save responses to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        responses_dict (dict): A dictionary containing the responses to be saved.\n",
    "        output_dir (str, optional): The directory where the JSON file will be saved. Defaults to '.'.\n",
    "        base_filename (str, optional): The base filename for the JSON file. Defaults to 'NO_FILE_NAME_RMIT'.\n",
    "        increment_filename (bool, optional): Whether to increment the filename if it already exists. Defaults to True.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If responses_dict is not a dictionary.\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(responses_dict, dict):\n",
    "        raise ValueError(\"responses_dict must be a dictionary.\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = f\"{base_filename}.json\" if not increment_filename else get_next_filename(output_dir, base_filename)\n",
    "    full_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    # Convert dictionary to list for JSON dumping\n",
    "    responses_list = list(responses_dict.values())\n",
    "\n",
    "    try:\n",
    "        with open(full_path, 'w') as file:\n",
    "            json.dump(responses_list, file, indent=4)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Failed to save data: {e}\")\n",
    "\n",
    "def get_next_filename(output_dir, base_filename):\n",
    "    \"\"\"\n",
    "    Get the next available filename for a given base filename in the specified output directory.\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): The directory where the files are stored.\n",
    "        base_filename (str): The base filename to be used.\n",
    "\n",
    "    Returns:\n",
    "        str: The next available filename in the format \"{base_filename}_{number}.json\".\n",
    "\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"{re.escape(base_filename)}_(\\d+).json\")\n",
    "    max_number = 0\n",
    "    for filename in os.listdir(output_dir):\n",
    "        match = pattern.match(filename)\n",
    "        if match:\n",
    "            number = int(match.group(1))\n",
    "            if number > max_number:\n",
    "                max_number = number\n",
    "    return f\"{base_filename}_{max_number + 1}.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_response(session, tweet, api_key, eval, prompt, df, idx, task):\n",
    "    \"\"\"\n",
    "    Fetches a response from the OpenAI API based on the given parameters.\n",
    "\n",
    "    Args:\n",
    "        session (aiohttp.ClientSession): The aiohttp client session.\n",
    "        tweet (str): The tweet for which the response is requested.\n",
    "        api_key (str): The API key for accessing the OpenAI API.\n",
    "        eval (str): The evaluation mode ('hard' or 'soft').\n",
    "        prompt (str): The prompt value for the conversation.\n",
    "        df (pandas.DataFrame): The DataFrame containing study levels and gender information.\n",
    "        idx (int): The index of the current row in the DataFrame.\n",
    "        task (int): The task number.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the tweet and the response from the API.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during the API request.\n",
    "\n",
    "    \"\"\"\n",
    "    api_key = API_KEY\n",
    "    delimiter = \"####\"\n",
    "    response_value = \"\"\n",
    "    prompt_value = f\"\"\"\"sexism, prejudice or discrimination based on sex or gender, especially against women and girls. Although its origin is unclear, the term sexism emerged from the “second-wave” feminism of the 1960s through ’80s and was most likely modeled on the civil rights movement’s term racism (prejudice or discrimination based on race). Sexism can be a belief that one sex is superior to or more valuable than another sex. It imposes limits on what men and boys can and should do and what women and girls can and should do. The concept of sexism was originally formulated to raise consciousness about the oppression of girls and women, although by the early 21st century it had sometimes been expanded to include the oppression of any sex, including men and boys, intersex people, and transgender people. You are a robot who detects sexism from text given in the prompt.\"\"\"    \n",
    "    column_value = f\"\"\"For each response, consider the perspective of individuals representing the following study levels: {df.study_levels_annotators[idx]} and gender: {df.gender_annotators[idx]}.\"\"\"\n",
    "    if eval == 'hard':\n",
    "        if task == 1:\n",
    "            response_value = f\"\"\"Give me 1 answer with [NO] or [YES]\"\"\"\n",
    "        elif task == 2:\n",
    "            response_value = f\"\"\"Give me 1 answer: [NO], [DIRECT], [REPORTED] or [JUDGEMENTAL]\"\"\"\n",
    "        elif task == 3:\n",
    "            response_value = f\"\"\"Give me 1 to 5 answers using commas for each answer. If it is sexist, classify it. Do not say YES. This is a multi-label task, so that more than one of the following labels may be assigned to each tweet: [NO], [IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMINANCE], [OBJECTIFICATION], [SEXUAL-VIOLENCE], or [MISOGYNY-NON-SEXUAL-VIOLENCE]\"\"\"\n",
    "            \n",
    "    if eval == 'soft':\n",
    "        if task == 1:\n",
    "            response_value = f\"\"\"Give me 6 answers with NO or YES. Format: [NO], [YES]\"\"\"\n",
    "        elif task == 2:\n",
    "            response_value = f\"\"\"Give me 6 answers with NO, DIRECT, REPORTED or JUDGEMENTAL using commas for each answer. Example: [NO], [DIRECT], [REPORTED], [JUDGEMENTAL], [JUDGEMENTAL], [NO]\"\"\"\n",
    "        elif task == 3:\n",
    "            response_value = f\"\"\"Give me 6 answers with NO, IDEOLOGICAL-INEQUALITY, STEREOTYPING-DOMINANCE, OBJECTIFICATION, SEXUAL-VIOLENCE, or MISOGYNY-NON-SEXUAL-VIOLENCE using commas for each answer. Example: [NO], [IDEOLOGICAL-INEQUALITY], [STEREOTYPING-DOMINANCE], [OBJECTIFICATION], [SEXUAL-VIOLENCE], [MISOGYNY-NON-SEXUAL-VIOLENCE]\"\"\"\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": f\"\"\"\"{prompt_value} {column_value} {response_value}\"\"\"},\n",
    "            # {\"role\": \"system\", \"content\": f\"\"\"\"{prompt_value} {response_value}\"\"\"},\n",
    "            {\"role\": \"user\", \"content\": delimiter + \" \" + tweet + delimiter + \" \"}\n",
    "        ]\n",
    "    }\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    try:\n",
    "        while True:\n",
    "            async with session.post('https://api.openai.com/v1/chat/completions', json=payload, headers=headers) as response:\n",
    "                if response.status == 429:\n",
    "                    retry_after = float(response.headers.get('Retry-After', 0.12))\n",
    "                    await asyncio.sleep(retry_after)\n",
    "                    continue\n",
    "                if response.status != 200:\n",
    "                    error_message = await response.text()\n",
    "                    print(f\"HTTP error {response.status}: {error_message}\")\n",
    "                    return {\"tweet\": tweet, \"response\": f\"HTTP error {response.status}: {error_message}\"}\n",
    "                result = await response.json()\n",
    "                if 'choices' in result and result['choices']:\n",
    "                    return {\"tweet\": tweet, \"response\": result['choices'][0]['message']['content']}\n",
    "                    print('49')\n",
    "                else:\n",
    "                    print(\"Unexpected response structure:\", json.dumps(result, indent=4))\n",
    "                    return {\"tweet\": tweet, \"response\": \"Error: Unexpected API response\"}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return {\"tweet\": tweet, \"response\": f\"Error: {str(e)}\"}\n",
    "        \n",
    "\n",
    "\n",
    "def normalize_response(response, eval, task):\n",
    "    \"\"\"\n",
    "    Normalize the response based on the evaluation method and task.\n",
    "\n",
    "    Args:\n",
    "        response (str): The response to be normalized.\n",
    "        eval (str): The evaluation method ('soft' or 'hard').\n",
    "        task (int): The task number (1, 2, or 3).\n",
    "\n",
    "    Returns:\n",
    "        dict or list: If eval is 'soft', returns a dictionary with normalized counts of response categories.\n",
    "                      If eval is 'hard', returns a list of normalized response categories.\n",
    "\n",
    "    \"\"\"\n",
    "    answers = response.replace('[', '').replace(']', '').split(',')\n",
    "    answers = [answer.strip().upper() for answer in answers]  # Clean and normalize case\n",
    "    # Define response categories based on the task and evaluation method\n",
    "    if eval == 'soft':\n",
    "        if task == 1:\n",
    "            counts = {'NO': 0, 'YES': 0}\n",
    "        elif task == 2:\n",
    "            counts = {'NO': 0, 'DIRECT': 0, 'REPORTED': 0, 'JUDGEMENTAL': 0}\n",
    "        elif task == 3:\n",
    "            counts = {'NO': 0, 'IDEOLOGICAL-INEQUALITY': 0, 'STEREOTYPING-DOMINANCE': 0, 'OBJECTIFICATION': 0,'SEXUAL-VIOLENCE':0, 'MISOGYNY-NON-SEXUAL-VIOLENCE':0}\n",
    "        else:\n",
    "            # Return empty dictionary for unspecified tasks or evaluation modes\n",
    "            print(answers)\n",
    "            return {}\n",
    "\n",
    "        # Count responses\n",
    "        for answer in answers:\n",
    "            if answer in counts:\n",
    "                counts[answer] += 1\n",
    "\n",
    "        # Normalize counts by the total number of responses\n",
    "        total = sum(counts.values())\n",
    "        if total > 0:\n",
    "            for key in counts:\n",
    "                counts[key] = counts[key] / total\n",
    "        return counts\n",
    "    else:\n",
    "        return answers\n",
    "\n",
    "async def main(**kwargs):\n",
    "    \"\"\"\n",
    "    Fetches and processes tweets asynchronously.\n",
    "\n",
    "    Args:\n",
    "        **kwargs: Keyword arguments containing the following parameters:\n",
    "            - dataframe: The dataframe containing the tweets.\n",
    "            - api_key: The API key for accessing the API.\n",
    "            - prompt: The prompt for generating responses.\n",
    "            - eval: The evaluation mode for the response.\n",
    "            - task: The task number for processing the tweets.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two dictionaries:\n",
    "        - responses_dict: A dictionary mapping row IDs to normalized response values.\n",
    "        - raw_responses: A dictionary mapping row IDs to raw response values.\n",
    "    \"\"\"\n",
    "    tweets = kwargs.get('dataframe')['tweet'].to_list()\n",
    "    api_key = kwargs.get('api_key')\n",
    "    prompt = kwargs.get('prompt')\n",
    "    eval = kwargs.get('eval')\n",
    "    df = kwargs.get('dataframe')\n",
    "    task = kwargs.get('task')\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        responses_dict = {}\n",
    "        raw_responses = {}  # Dictionary to store non-normalized responses\n",
    "        batch_size = 500  # Define the number of requests after which to pause\n",
    "        requests_per_second = batch_size / 60  # Calculate the delay needed between each request to adhere to rate limits\n",
    "        for i, tweet in enumerate(tweets):\n",
    "            # Pause execution to respect rate limits after every batch of 500 requests\n",
    "            if i % batch_size == 0 and i != 0:\n",
    "                print(f\"Processed {i} tweets, sleeping for 60 seconds to respect rate limits...\")\n",
    "                await asyncio.sleep(60)\n",
    "\n",
    "            # Fetch and process each tweet\n",
    "            if task == 1:\n",
    "                response = await fetch_response(session, tweet, api_key, eval, prompt, df, i, task)\n",
    "                if 'response' in response:\n",
    "                    normalized_values = normalize_response(response['response'], eval, task)\n",
    "                    row_id = df[df['tweet'] == tweet]['id_EXIST'].values[0]\n",
    "                    responses_dict[row_id] = {\n",
    "                        'id': row_id,\n",
    "                        'value': normalized_values,\n",
    "                        'test_case': \"EXIST2024\"\n",
    "                    }\n",
    "                    # Store raw response\n",
    "                    raw_responses[row_id] = {\n",
    "                        'id': row_id,\n",
    "                        'response': response['response'],\n",
    "                        'test_case': \"EXIST2024\"\n",
    "                    }\n",
    "                else:\n",
    "                    print(f\"Error or unexpected format in response for tweet ID {tweet}: {response}\")\n",
    "            if task == 2:\n",
    "                response = await fetch_response(session, tweet, api_key, eval, prompt, df, i, task)\n",
    "                if 'response' in response:\n",
    "                    normalized_values = normalize_response(response['response'], eval, task)\n",
    "                    row_id = df[df['tweet'] == tweet]['id_EXIST'].values[0]\n",
    "                    responses_dict[row_id] = {\n",
    "                        'id': row_id,\n",
    "                        'value': normalized_values,\n",
    "                        'test_case': \"EXIST2024\"\n",
    "                    }\n",
    "            if task == 3:\n",
    "                response = await fetch_response(session, tweet, api_key, eval, prompt, df, i, task)\n",
    "                if 'response' in response:\n",
    "                    normalized_values = normalize_response(response['response'], eval, task)\n",
    "                    row_id = df[df['tweet'] == tweet]['id_EXIST'].values[0]\n",
    "                    responses_dict[row_id] = {\n",
    "                        'id': row_id,\n",
    "                        'value': normalized_values,\n",
    "                        'test_case': \"EXIST2024\"\n",
    "                    }\n",
    "                    \n",
    "                # Sleep to spread requests evenly across the rate limit period\n",
    "                await asyncio.sleep(1 / requests_per_second)\n",
    "\n",
    "        return responses_dict, raw_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_location = 'dev/EXIST2024_dev.json' # Test model\n",
    "json_location = 'test/EXIST2023_test_clean.json' # For Official Submission\n",
    "test_case = \"EXIST2024\"\n",
    "\n",
    "df = json_df(json_location, 'df')\n",
    "df = df.head(2)\n",
    "\"\"\"*****After running task 3, make sure to change the prompt to fit set 2*****\"\"\"\n",
    "test_params = {\n",
    "    'task': 3, # 1 or 2 and then 3!?\n",
    "    'eval': 'hard', # soft or hard\n",
    "    'prompt': 'Prompts/prompt.txt', # If you would like to change the prompt from a text file instead of in the code.\n",
    "    'api_key': API_KEY,\n",
    "    'dataframe': df\n",
    "    }\n",
    "base_filename = f\"task{test_params['task']}_{test_params['eval']}_RMITIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    responses, raw_responses = loop.run_until_complete(main(**test_params))\n",
    "\n",
    "    # Saving the normalized responses if they exist\n",
    "    if responses:\n",
    "        save_responses(responses, output_dir='test_formats/gpt4/test', base_filename=base_filename, increment_filename=True)\n",
    "\n",
    "    # Optionally save the raw responses too\n",
    "    if raw_responses:\n",
    "        save_responses(raw_responses, output_dir='raw_formats/gpt4/test', base_filename=base_filename, increment_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID Match Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to load JSON data from a file\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Function to find mismatched values based on ID\n",
    "def compare_values(data1, data2):\n",
    "    mismatch = []  # List to collect IDs where values do not match\n",
    "    # Create a dictionary from the second dataset for quick lookup\n",
    "    data2_dict = {item['id']: item['value'] for item in data2}\n",
    "    # Iterate through the first dataset and compare\n",
    "    for item in data1:\n",
    "        id = item['id']\n",
    "        # Check if the ID exists in both datasets and the values do not match\n",
    "        if id in data2_dict and item['value'] != data2_dict[id]:\n",
    "            mismatch.append(id)  # Add to mismatch list\n",
    "    return mismatch\n",
    "\n",
    "# Load data from files (replace 'file1.json' and 'file2.json' with your actual file paths)\n",
    "data1 = load_json('file1')\n",
    "data2 = load_json('file2')\n",
    "# Compare the files and get IDs with the same value\n",
    "matching_ids = compare_values(data1, data2)\n",
    "\n",
    "# Output the results\n",
    "print(\"IDs Mismatched:\", matching_ids)\n",
    "print(len(matching_ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
