# RMIT-IR at EXIST Lab at CLEF 2024

*Under construction...*

- [Tasks 1-3:](./Task1-3) Code for the zero-shot In-Context Learning with off-the-shelf pre-trained Large Language Models
- [Task 4:](./task4) Code for the multimodal contrastive learning runs for Task 4 (Sexism identification in Memes). 

Code for the RMIT-IR participation at [EXIST Lab at CLEF 2024](http://nlp.uned.es/exist2024/)

### Abstract
This paper describes RMIT-IR team’s participation in the EXIST Lab at CLEF 2024. The proposed approaches aim to address sexism characterization on microblog posts (Tasks 1, 2, and 3) and sexism identification on memes (Task 4). For Tasks 1-3, we studied the effectiveness of zero-shot In-Context Learning (ICL) with off-the-shelf pre-trained Large Language Models (LLMs) to mimic the scenario of minimal intervention of a practitioner aiming to build sexism characterization systems. Our approaches for meme classification (Task 4) utilize CLIP (Contrastive Language-Image Pre-training) to experiment with multi-modal embeddings and zero-shot sexism identification models. We report the performance of our approaches under the learning with disagreements regime (Soft evaluation) and also for label predictions (Hard evaluation). 

### Citation

Tony Kim Smith, H Ruda Nie, Johanne R. Trippas, Damiano Spina (2024). [RMIT-IR at EXIST Lab at CLEF 2024](https://www.damianospina.com/publication/smith-2024-rmit/smith-2024-rmit.pdf). Working Notes of CLEF 2024 – Conference and Labs of the Evaluation Forum.

```bibtex
@inproceedings{smith2024rmit,
 author = {Smith, Tony Kim and Nie, H Ruda and Trippas, Johanne R. and Spina, Damiano},
 booktitle = {Working Notes of CLEF 2024 -- Conference and Labs of the Evaluation Forum},
 title = {RMIT-IR at EXIST Lab at CLEF 2024},
 year = {2024}
}
```
